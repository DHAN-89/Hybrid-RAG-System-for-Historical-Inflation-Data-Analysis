# Hybrid-RAG-System-for-Historical-Inflation-Data-Analysis
This work presents a comprehensive solution for analyzing historical inflation data.
This notebook presents a comprehensive solution for analyzing historical inflation data and responding to user queries using a hybrid Retrieval-Augmented Generation (RAG) system. The project begins with loading and meticulously preprocessing an Excel dataset containing annual inflation figures. It then transforms this structured data into text chunks, which are subsequently embedded into a vector space using Sentence-BERT. A FAISS index is constructed to enable efficient retrieval of relevant information.
The core innovation lies in its RAG system, which is enhanced with dedicated analytical helper functions. This design enables the system to accurately and deterministically answer specific numerical queries (e.g., highest/lowest inflation, trends, averages) by performing calculations directly on the pandas DataFrame. For more open-ended or contextual questions, the system seamlessly falls back to a traditional RAG pipeline, retrieving relevant information from the FAISS index and using a GPT-2B-it large language model to generate grounded responses. This approach ensures both numerical precision for analytical tasks and flexible, context-aware answers for broader inquiries, effectively tackling the challenge of providing insightful and accurate information from historical economic data. In this context, "hybrid RAG" refers to the system's ability to combine two different approaches to answer user queries:
Direct Analytical Functions: For specific, data-centric questions (like "What is the highest inflation year?" or "What is the average inflation between X and Y?"), the system first attempts to answer them using pre-defined Python functions. These functions directly query and calculate results from the pandas DataFrame. This approach ensures numerical precision and deterministic answers for questions that can be solved with direct data analysis.
Retrieval-Augmented Generation (Traditional RAG): If a query doesn't fit one of the pre-defined analytical patterns, the system falls back to a more conventional RAG pipeline. This involves:
Retrieval: Searching a database of text embeddings (generated from your inflation data) to find the most relevant information chunks. Augmentation: Feeding these retrieved chunks as context to a Large Language Model (LLM). Generation: The LLM then uses this context, along with your original query, to generate a human-like answer. This approach provides flexibility and contextual understanding for broader, more qualitative questions. So, it's "hybrid" because it doesn't solely rely on the LLM to interpret and answer everything. Instead, it intelligently routes queries to the most appropriate method – either a precise analytical calculation or a contextual LLM generation – optimizing for both accuracy and versatility.
findings & trade-offs for my decisions
Key Findings: Data Quality and Completeness: The data loading and preprocessing steps successfully cleaned the raw Excel file. Importantly, no missing values were found for the Annual_Inflation column, ensuring a complete dataset for analysis. Inflation Trends and Extremes: The analysis clearly identified the highest inflation year (2022 at 286.75) and the lowest (1914 at 10.02). Specific periods like 1939-1945, 2000-2010, and 2019-2021 consistently showed increasing inflation trends.
Accurate Analytical Responses: The dedicated analytical helper functions provided precise and accurate numerical answers for queries such as highest/lowest inflation, trends over specific periods, and average inflation (e.g., 151.94 between 1990 and 2000). This confirmed the effectiveness of the direct calculation approach.
Effective Contextual Handling: For questions not directly answerable by the analytical functions, the RAG system successfully leveraged the LLM. It also correctly identified when insufficient context was available (e.g., for 'inflation spikes' or 'pandemic times' beyond the explicitly provided data points), preventing hallucination.
Robust Summary Statistics: Comprehensive summary statistics for Annual_Inflation were generated, providing a clear overview of the data's distribution (mean, std, min, max, quartiles).
Trade-offs in Design Decisions: Hybrid RAG Approach (Analytical vs. LLM):
Benefit: This hybrid design offers the best of both worlds. It ensures high accuracy and determinism for analytical queries by relying on direct Python computations, eliminating LLM-induced errors or hallucinations for numerical facts. Simultaneously, it provides flexibility and contextual understanding for broader questions by leveraging the LLM. Trade-off: The complexity of implementation is higher. It requires careful query classification and the development of specific analytical functions. This adds maintenance overhead compared to a purely LLM-driven RAG system, which might simplify the architecture but potentially sacrifice numerical precision. Choice of LLM (Gemma-2B-it):
Benefit: Gemma-2B-it is a relatively small model, offering faster inference times and lower computational resource requirements (GPU memory). This makes it more practical for deployment in resource-constrained environments like Colab and potentially for real-time applications.
Trade-off: Being a smaller model, Gemma-2B-it might possess less nuanced understanding and lower complex reasoning capabilities compared to much larger, more powerful LLMs. Its performance is highly dependent on the quality and specificity of the retrieved context.
Embedding Model (Sentence-BERT: all-MiniLM-L6-v2):
Benefit: all-MiniLM-L6-v2 is known for its efficiency, speed, and good performance in generating semantically meaningful embeddings. It strikes a good balance between speed and quality for general-purpose sentence embeddings. Trade-off: While generally strong, it might not capture highly domain-specific nuances as effectively as a larger or fine-tuned model could. However, for this dataset, its performance proved adequate. FAISS Indexing (IndexFlatIP):
Benefit: FAISS provides extremely fast similarity search capabilities, crucial for quick retrieval of relevant context. IndexFlatIP is efficient for cosine similarity when embeddings are normalized. Trade-off: Primarily designed for in-memory indexing, which means the entire index must reside in RAM. For extremely large datasets (beyond what's typically handled in a notebook), memory constraints could become a limitation, necessitating more advanced FAISS index types or distributed solutions.



